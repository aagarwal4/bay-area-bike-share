{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row, SQLContext\n",
    "from pyspark.sql.types  import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, RegressionEvaluator\n",
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "statusDF = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"uri\", \"mongodb://127.0.0.1/msan697.status\").load()\n",
    "\n",
    "stationDF = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"uri\", \"mongodb://127.0.0.1/msan697.station\").load()\n",
    "\n",
    "weatherDF = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"uri\", \"mongodb://127.0.0.1/msan697.weather\").load()\n",
    "\n",
    "tripDF = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"uri\", \"mongodb://127.0.0.1/msan697.trip\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- dock_count: integer (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      " |-- installation_date: string (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- long: double (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stationDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Status Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "statusDF = statusDF.withColumn('dayofweek',date_format(from_unixtime(unix_timestamp(statusDF[\"time\"][0:10], 'yyyy/MM/dd')),'EEEE'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "statusDF = statusDF.withColumn(\"dayofweek\", \n",
    "                    when(col(\"dayofweek\").isNull(), date_format(from_unixtime(unix_timestamp(statusDF[\"time\"][0:10], 'yyyy-MM-dd')),'EEEE')).\n",
    "                        otherwise(col('dayofweek')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Adding weekend column\n",
    "statusDF = statusDF.withColumn(\"weekend\", when(col('dayofweek') == 'Saturday',1).when(col('dayofweek') == 'Sunday', 1).otherwise(0))\n",
    "#Adding weekday column\n",
    "statusDF = statusDF.withColumn(\"weekday\", when(col('dayofweek') == 'Saturday',0).when(col('dayofweek') == 'Sunday', 0).otherwise(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Adding hourofday column\n",
    "statusDF = statusDF.withColumn('hourofday',statusDF[\"time\"][12:2].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Adding morning column\n",
    "statusDF = statusDF.withColumn(\"morning\", when(col('hourofday').between(5,11),1).otherwise(0))\n",
    "#Adding afternoon column\n",
    "statusDF = statusDF.withColumn(\"afternoon\", when(col('hourofday').between(12,16),1).otherwise(0))\n",
    "#Adding evening column\n",
    "statusDF = statusDF.withColumn(\"evening\", when(col('hourofday').between(17,22),1).otherwise(0))\n",
    "#Adding night column\n",
    "statusDF = statusDF.withColumn(\"night\", when(col('hourofday').between(23,24), 1).when(col('hourofday').between(0,4),1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Adding month column\n",
    "statusDF = statusDF.withColumn('month',month(from_unixtime(unix_timestamp(statusDF[\"time\"][0:10], 'yyyy/MM/dd'))))\n",
    "# Adding year column\n",
    "statusDF = statusDF.withColumn('year',year(from_unixtime(unix_timestamp(statusDF[\"time\"][0:10], 'yyyy/MM/dd'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning month column\n",
    "statusDF = statusDF.withColumn(\"month\", \n",
    "                    when(col(\"month\").isNull(), month(from_unixtime(unix_timestamp(statusDF[\"time\"][0:10], 'yyyy-MM-dd')))).\n",
    "                        otherwise(col('month')))\n",
    "#Cleaning year column\n",
    "statusDF = statusDF.withColumn(\"year\", \n",
    "                    when(col(\"year\").isNull(), year(from_unixtime(unix_timestamp(statusDF[\"time\"][0:10], 'yyyy-MM-dd')))).\n",
    "                        otherwise(col('year')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "statusDF_avg = sqlContext.sql(\"\"\"\n",
    "SELECT station_id, weekend, weekday, hourofday, month, year, dayofweek, morning, afternoon, evening, night,\n",
    "avg(bikes_available) AS avg_bikes_available, \n",
    "avg(docks_available) AS avg_docks_available\n",
    "FROM statusDF\n",
    "GROUP BY 1,2,3,4,5,6,7,8,9,10,11\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Weather Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weatherDF = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"uri\", \"mongodb://127.0.0.1/msan697.weather\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tocols_to_change = ['cloud_cover',\n",
    " 'events',\n",
    " 'max_dew_point_f',\n",
    " 'max_gust_speed_mph',\n",
    " 'max_humidity',\n",
    " 'max_sea_level_pressure_inches',\n",
    " 'max_temperature_f',\n",
    " 'max_visibility_miles',\n",
    " 'max_wind_Speed_mph',\n",
    " 'mean_dew_point_f',\n",
    " 'mean_humidity',\n",
    " 'mean_sea_level_pressure_inches',\n",
    " 'mean_temperature_f',\n",
    " 'mean_visibility_miles',\n",
    " 'mean_wind_speed_mph',\n",
    " 'min_dew_point_f',\n",
    " 'min_humidity',\n",
    " 'min_sea_level_pressure_inches',\n",
    " 'min_temperature_f',\n",
    " 'min_visibility_miles',\n",
    " 'precipitation_inches',\n",
    " 'wind_dir_degrees',\n",
    " 'zip_code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "weatherDF = weatherDF.select([*(col(c).cast(\"float\").alias(c) for c in tocols_to_change), 'date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weatherDF = weatherDF.withColumn(\"events\", when(col('events') == 'Fog', 1).\\\n",
    "                                 when(col('events').like ('%ain'),2).\\\n",
    "                                 when(col('events') == 'Fog-Rain',3).\\\n",
    "                                 when(col('events') == 'Rain-Thunderstorm',4).\\\n",
    "                                 otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Adding day of week column\n",
    "weatherDF = weatherDF.withColumn('dayofweek',date_format(from_unixtime(unix_timestamp(weatherDF[\"date\"], 'M/dd/yyyy')),'EEEE'))\n",
    "#Adding weekend column\n",
    "weatherDF = weatherDF.withColumn(\"weekend\", when(col('dayofweek') == 'Saturday',1).when(col('dayofweek') == 'Sunday', 1).otherwise(0))\n",
    "#Adding weekday column\n",
    "weatherDF = weatherDF.withColumn(\"weekday\", when(col('dayofweek') == 'Saturday',0).when(col('dayofweek') == 'Sunday', 0).otherwise(1))\n",
    "#Adding month column\n",
    "weatherDF = weatherDF.withColumn('month',month(from_unixtime(unix_timestamp(weatherDF[\"date\"], 'M/dd/yyyy'))))\n",
    "# Adding year column\n",
    "weatherDF = weatherDF.withColumn('year',year(from_unixtime(unix_timestamp(weatherDF[\"date\"], 'M/dd/yyyy'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "weatherDF = weatherDF.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(dayofweek='Monday', month=10, year=2014, weekend=0, weekday=1, max_temperature_f=76.6, mean_temperature_f=65.05, min_temperature_f=53.35, max_dew_point_f=58.65, mean_dew_point_f=51.4, min_dew_point_f=43.55, max_humidity=90.3, mean_humidity=65.4, min_humidity=38.5, max_sea_level_pressure_inches=30.015499973297118, mean_sea_level_pressure_inches=29.96599988937378, min_sea_level_pressure_inches=29.91900005340576, max_visibility_miles=11.25, mean_visibility_miles=10.05, min_visibility_miles=8.2, max_wind_Speed_mph=14.55, mean_wind_speed_mph=4.75, max_gust_speed_mph=16.55, precipitation_inches=0.0014999999664723873, cloud_cover=1.85, wind_dir_degrees=298.65, events=0.0)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weatherDF.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cloud_cover: float (nullable = false)\n",
      " |-- events: integer (nullable = false)\n",
      " |-- max_dew_point_f: float (nullable = false)\n",
      " |-- max_gust_speed_mph: float (nullable = false)\n",
      " |-- max_humidity: float (nullable = false)\n",
      " |-- max_sea_level_pressure_inches: float (nullable = false)\n",
      " |-- max_temperature_f: float (nullable = false)\n",
      " |-- max_visibility_miles: float (nullable = false)\n",
      " |-- max_wind_Speed_mph: float (nullable = false)\n",
      " |-- mean_dew_point_f: float (nullable = false)\n",
      " |-- mean_humidity: float (nullable = false)\n",
      " |-- mean_sea_level_pressure_inches: float (nullable = false)\n",
      " |-- mean_temperature_f: float (nullable = false)\n",
      " |-- mean_visibility_miles: float (nullable = false)\n",
      " |-- mean_wind_speed_mph: float (nullable = false)\n",
      " |-- min_dew_point_f: float (nullable = false)\n",
      " |-- min_humidity: float (nullable = false)\n",
      " |-- min_sea_level_pressure_inches: float (nullable = false)\n",
      " |-- min_temperature_f: float (nullable = false)\n",
      " |-- min_visibility_miles: float (nullable = false)\n",
      " |-- precipitation_inches: float (nullable = false)\n",
      " |-- wind_dir_degrees: float (nullable = false)\n",
      " |-- zip_code: float (nullable = false)\n",
      " |-- date: string (nullable = true)\n",
      " |-- dayofweek: string (nullable = true)\n",
      " |-- weekend: integer (nullable = false)\n",
      " |-- weekday: integer (nullable = false)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weatherDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "weatherDF = weatherDF.drop(\"date\").groupBy(\"dayofweek\", \"month\",\"year\",\"weekend\",\"weekday\")\\\n",
    ".agg(avg(\"max_temperature_f\").alias(\"max_temperature_f\"), \\\n",
    "avg(\"mean_temperature_f\").alias(\"mean_temperature_f\"),\\\n",
    "avg(\"min_temperature_f\").alias(\"min_temperature_f\"),\\\n",
    "avg(\"max_dew_point_f\").alias(\"max_dew_point_f\"),\\\n",
    "avg(\"mean_dew_point_f\").alias(\"mean_dew_point_f\"), \\\n",
    "avg(\"min_dew_point_f\").alias(\"min_dew_point_f\"),\\\n",
    "avg(\"max_humidity\").alias(\"max_humidity\"), \\\n",
    "avg(\"mean_humidity\").alias(\"mean_humidity\"),\\\n",
    "avg(\"min_humidity\").alias(\"min_humidity\"), \\\n",
    "avg(\"max_sea_level_pressure_inches\").alias(\"max_sea_level_pressure_inches\"),\\\n",
    "avg(\"mean_sea_level_pressure_inches\").alias(\"mean_sea_level_pressure_inches\"),\\\n",
    "avg(\"min_sea_level_pressure_inches\").alias(\"min_sea_level_pressure_inches\"),\\\n",
    "avg(\"max_visibility_miles\").alias(\"max_visibility_miles\"), \\\n",
    "avg(\"mean_visibility_miles\").alias(\"mean_visibility_miles\"),\\\n",
    "avg(\"min_visibility_miles\").alias(\"min_visibility_miles\"), \\\n",
    "avg(\"max_wind_Speed_mph\").alias(\"max_wind_Speed_mph\"),\\\n",
    "avg(\"mean_wind_speed_mph\").alias(\"mean_wind_speed_mph\"),\\\n",
    "avg(\"max_gust_speed_mph\").alias(\"max_gust_speed_mph\"),\\\n",
    "avg(\"precipitation_inches\").alias(\"precipitation_inches\"),\\\n",
    "avg(\"cloud_cover\").alias(\"cloud_cover\"),\\\n",
    "avg(\"wind_dir_degrees\").alias(\"wind_dir_degrees\"),\\\n",
    "avg(\"events\").alias(\"events\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.sql('drop table if exists weatherDF')\n",
    "weatherDF.write.saveAsTable('weatherDF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stations Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add age of the docks\n",
    "stationDF = stationDF.withColumn('age', \\\n",
    "               datediff(from_unixtime(unix_timestamp(date_format(current_date(), \"M/d/y\"), 'MM/dd/yyy')),\\\n",
    "                              from_unixtime(unix_timestamp(stationDF['installation_date'], 'MM/dd/yyy'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlContext.sql('drop table if exists stationDF')\n",
    "stationDF.write.saveAsTable('stationDF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trip Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tripDF = tripDF.withColumn('start_date', concat(col('start_date'),lit(':00'))).withColumn('end_date', concat(col('end_date'),lit(':00')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tripDF = tripDF.withColumn('dayofweek',date_format(from_unixtime(unix_timestamp('start_date', 'MM/dd/yyy HH:mm:ss')),'EEEE'))\\\n",
    ".withColumn(\"weekend\", when(col('dayofweek') == 'Saturday',1).when(col('dayofweek') == 'Sunday', 1).otherwise(0))\\\n",
    ".withColumn(\"weekday\", when(col('dayofweek') == 'Saturday',0).when(col('dayofweek') == 'Sunday', 0).otherwise(1))\\\n",
    ".withColumn('hourofday',hour(from_unixtime(unix_timestamp('start_date', 'MM/dd/yyy HH:mm:ss'))))\\\n",
    ".withColumn('month',month(from_unixtime(unix_timestamp('start_date', 'MM/dd/yyy HH:mm:ss'))))\\\n",
    ".withColumn('year',year(from_unixtime(unix_timestamp('start_date', 'MM/dd/yyy HH:mm:ss'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tripDF = tripDF.withColumn('start_date_part', date_format(to_date(from_unixtime(unix_timestamp('start_date', 'MM/dd/yyy HH:mm:ss'))), 'MM/dd/yyy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tripDF = tripDF.withColumn('end_date_part', date_format(to_date(from_unixtime(unix_timestamp('end_date', 'MM/dd/yyy HH:mm:ss'))), 'MM/dd/yyy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.sql('drop table if exists tripDF')\n",
    "tripDF.write.saveAsTable('tripDF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tripDF_final = sqlContext.sql(\"\"\"\n",
    "select a.start_station_id, a.hourofday,\n",
    "        weekend, weekday, month, year, dayofweek,\n",
    "        avg(outgoing_bikes) as outgoing_bikes, avg(incoming_bikes) as incoming_bikes,\n",
    "        (coalesce(avg(outgoing_bikes),0) - coalesce(avg(incoming_bikes),0)) as traffic\n",
    "        from\n",
    "        (\n",
    "            select hourofday, start_station_id, start_date_part, weekend, weekday, month, year, dayofweek,\n",
    "            sum(1) as outgoing_bikes\n",
    "            from tripDF\n",
    "            group by 1,2,3,4,5,6,7,8\n",
    "        ) as a\n",
    "        left join\n",
    "        (\n",
    "            select hourofday, end_station_id, end_date_part, sum(1) as incoming_bikes\n",
    "            from tripDF\n",
    "            group by 1,2,3\n",
    "        ) as b\n",
    "        on a.start_station_id = b.end_station_id and a.hourofday = b.hourofday and a.start_date_part = b.end_date_part\n",
    "        group by 1,2,3,4,5,6,7\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- start_station_id: integer (nullable = true)\n",
      " |-- hourofday: integer (nullable = true)\n",
      " |-- weekend: integer (nullable = true)\n",
      " |-- weekday: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- dayofweek: string (nullable = true)\n",
      " |-- outgoing_bikes: double (nullable = true)\n",
      " |-- incoming_bikes: double (nullable = true)\n",
      " |-- traffic: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tripDF_final.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlContext.sql('drop table if exists tripDF_final')\n",
    "tripDF_final.write.saveAsTable('tripDF_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = stationDF.join(tripDF_final, stationDF.id == tripDF_final.start_station_id, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "joined_df = joined_df.drop(\"start_station_id\", \"end_station_id\", \"_id\", \"installation_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "joined_df = joined_df.withColumnRenamed(\"id\", \"station_id\").withColumnRenamed(\"name\", \"station_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "joined_df2 = joined_df.join(statusDF_avg, [\"station_id\", \"hourofday\", \"dayofweek\", \"weekend\", \"weekday\", \"month\", \"year\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "joined_df2 = joined_df2.withColumnRenamed(\"dock_count\", \"total_capacity\").withColumnRenamed(\"age\", \"station_age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(dayofweek='Sunday', month=9, year=2013, weekend=1, weekday=0, max_temperature_f=77.12, mean_temperature_f=67.08, min_temperature_f=57.24, max_dew_point_f=59.4, mean_dew_point_f=55.36, min_dew_point_f=51.16, max_humidity=86.92, mean_humidity=69.36, min_humidity=46.36, max_sea_level_pressure_inches=29.9747998046875, mean_sea_level_pressure_inches=29.940800170898438, min_sea_level_pressure_inches=29.852399826049805, max_visibility_miles=10.0, mean_visibility_miles=10.0, min_visibility_miles=9.84, max_wind_Speed_mph=17.28, mean_wind_speed_mph=6.0, max_gust_speed_mph=20.6, precipitation_inches=0.00039999999105930326, cloud_cover=1.92, wind_dir_degrees=312.68, events=0.0)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weatherDF.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_joined = joined_df2.join(weatherDF, [ \"dayofweek\", \"weekend\", \"weekday\", \"month\", \"year\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.sql('drop table if exists final_features')\n",
    "final_joined.write.saveAsTable('final_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_nodup = final_joined.distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlContext.sql('drop table if exists final_no_duplicates')\n",
    "final_nodup.write.saveAsTable('final_no_duplicates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_nodup = sqlContext.sql('select * from final_no_duplicates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dayofweek: string (nullable = true)\n",
      " |-- weekend: integer (nullable = true)\n",
      " |-- weekday: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- station_id: integer (nullable = true)\n",
      " |-- hourofday: integer (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- total_capacity: integer (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- long: double (nullable = true)\n",
      " |-- station_name: string (nullable = true)\n",
      " |-- station_age: integer (nullable = true)\n",
      " |-- outgoing_bikes: double (nullable = true)\n",
      " |-- incoming_bikes: double (nullable = true)\n",
      " |-- traffic: double (nullable = true)\n",
      " |-- morning: integer (nullable = true)\n",
      " |-- afternoon: integer (nullable = true)\n",
      " |-- evening: integer (nullable = true)\n",
      " |-- night: integer (nullable = true)\n",
      " |-- avg_bikes_available: double (nullable = true)\n",
      " |-- avg_docks_available: double (nullable = true)\n",
      " |-- max_temperature_f: double (nullable = true)\n",
      " |-- mean_temperature_f: double (nullable = true)\n",
      " |-- min_temperature_f: double (nullable = true)\n",
      " |-- max_dew_point_f: double (nullable = true)\n",
      " |-- mean_dew_point_f: double (nullable = true)\n",
      " |-- min_dew_point_f: double (nullable = true)\n",
      " |-- max_humidity: double (nullable = true)\n",
      " |-- mean_humidity: double (nullable = true)\n",
      " |-- min_humidity: double (nullable = true)\n",
      " |-- max_sea_level_pressure_inches: double (nullable = true)\n",
      " |-- mean_sea_level_pressure_inches: double (nullable = true)\n",
      " |-- min_sea_level_pressure_inches: double (nullable = true)\n",
      " |-- max_visibility_miles: double (nullable = true)\n",
      " |-- mean_visibility_miles: double (nullable = true)\n",
      " |-- min_visibility_miles: double (nullable = true)\n",
      " |-- max_wind_Speed_mph: double (nullable = true)\n",
      " |-- mean_wind_speed_mph: double (nullable = true)\n",
      " |-- max_gust_speed_mph: double (nullable = true)\n",
      " |-- precipitation_inches: double (nullable = true)\n",
      " |-- cloud_cover: double (nullable = true)\n",
      " |-- wind_dir_degrees: double (nullable = true)\n",
      " |-- events: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_nodup.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#converting strings to numeric values\n",
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def indexStringColumns(df, cols):\n",
    "    #variable newdf will be updated several times\n",
    "    newdf = df\n",
    "    \n",
    "    for c in cols:\n",
    "        #For each given colum, fits StringIndexerModel.\n",
    "        si = StringIndexer(inputCol=c, outputCol=c+\"-num\")\n",
    "        sm = si.fit(newdf)\n",
    "        #Creates a DataFame by putting the transformed values in the new colum with suffix \"-num\" \n",
    "        #and then drops the original columns.\n",
    "        #and drop the \"-num\" suffix. \n",
    "        newdf = sm.transform(newdf).drop(c)\n",
    "        newdf = newdf.withColumnRenamed(c+\"-num\", c)\n",
    "    return newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_nodup_numeric = indexStringColumns(final_nodup, [\"dayofweek\", \"city\", \"station_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- weekend: integer (nullable = true)\n",
      " |-- weekday: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- station_id: integer (nullable = true)\n",
      " |-- hourofday: integer (nullable = true)\n",
      " |-- total_capacity: integer (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- long: double (nullable = true)\n",
      " |-- station_age: integer (nullable = true)\n",
      " |-- outgoing_bikes: double (nullable = true)\n",
      " |-- incoming_bikes: double (nullable = true)\n",
      " |-- traffic: double (nullable = true)\n",
      " |-- morning: integer (nullable = true)\n",
      " |-- afternoon: integer (nullable = true)\n",
      " |-- evening: integer (nullable = true)\n",
      " |-- night: integer (nullable = true)\n",
      " |-- avg_bikes_available: double (nullable = true)\n",
      " |-- avg_docks_available: double (nullable = true)\n",
      " |-- max_temperature_f: double (nullable = true)\n",
      " |-- mean_temperature_f: double (nullable = true)\n",
      " |-- min_temperature_f: double (nullable = true)\n",
      " |-- max_dew_point_f: double (nullable = true)\n",
      " |-- mean_dew_point_f: double (nullable = true)\n",
      " |-- min_dew_point_f: double (nullable = true)\n",
      " |-- max_humidity: double (nullable = true)\n",
      " |-- mean_humidity: double (nullable = true)\n",
      " |-- min_humidity: double (nullable = true)\n",
      " |-- max_sea_level_pressure_inches: double (nullable = true)\n",
      " |-- mean_sea_level_pressure_inches: double (nullable = true)\n",
      " |-- min_sea_level_pressure_inches: double (nullable = true)\n",
      " |-- max_visibility_miles: double (nullable = true)\n",
      " |-- mean_visibility_miles: double (nullable = true)\n",
      " |-- min_visibility_miles: double (nullable = true)\n",
      " |-- max_wind_Speed_mph: double (nullable = true)\n",
      " |-- mean_wind_speed_mph: double (nullable = true)\n",
      " |-- max_gust_speed_mph: double (nullable = true)\n",
      " |-- precipitation_inches: double (nullable = true)\n",
      " |-- cloud_cover: double (nullable = true)\n",
      " |-- wind_dir_degrees: double (nullable = true)\n",
      " |-- events: double (nullable = true)\n",
      " |-- dayofweek: double (nullable = true)\n",
      " |-- city: double (nullable = true)\n",
      " |-- station_name: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_nodup_numeric.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_nodup_numeric = final_nodup_numeric.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Traffic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_nodup_numeric_traffic = final_nodup_numeric.drop('avg_bikes_available', 'avg_docks_available')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_cols = final_nodup_numeric_traffic.schema.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cols.remove('traffic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the data with Vector Assembler.\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "va = VectorAssembler(outputCol=\"features\", inputCols = input_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lpoints - labeled data.\n",
    "lpoints = va.transform(final_nodup_numeric_traffic).select(\"features\", \"traffic\").withColumnRenamed(\"traffic\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n",
      "|            features|              label|\n",
      "+--------------------+-------------------+\n",
      "|[0.0,1.0,4.0,2014...|               -0.5|\n",
      "|[0.0,1.0,4.0,2014...|-0.6666666666666667|\n",
      "|[0.0,1.0,4.0,2014...| 0.6666666666666667|\n",
      "|[0.0,1.0,4.0,2014...|                1.0|\n",
      "|[0.0,1.0,4.0,2014...|                1.0|\n",
      "|[0.0,1.0,4.0,2014...|                1.0|\n",
      "|[0.0,1.0,4.0,2014...|                1.0|\n",
      "|[0.0,1.0,4.0,2014...|               -0.5|\n",
      "|[0.0,1.0,4.0,2014...|                0.0|\n",
      "|[0.0,1.0,4.0,2014...|                0.0|\n",
      "|[0.0,1.0,4.0,2014...|               1.25|\n",
      "|[0.0,1.0,4.0,2014...|               -1.0|\n",
      "|[0.0,1.0,4.0,2014...|                0.0|\n",
      "|[0.0,1.0,4.0,2014...|                1.0|\n",
      "|[0.0,1.0,4.0,2014...|                0.5|\n",
      "|[0.0,1.0,4.0,2014...| 1.6666666666666667|\n",
      "|[0.0,1.0,4.0,2014...|  2.916666666666667|\n",
      "|[0.0,1.0,4.0,2014...|                1.0|\n",
      "|[0.0,1.0,4.0,2014...|                0.0|\n",
      "|[0.0,1.0,4.0,2014...|               0.75|\n",
      "+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lpoints.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide the dataset into training and testing sets.\n",
    "splits = lpoints.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cache() : the algorithm is interative and training and data sets are going to be reused many times.\n",
    "train_df = splits[0].cache()\n",
    "valid_df = splits[1].cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression for Traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model.\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "lrModel = lr.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.571816\n",
      "r2: 0.917037\n"
     ]
    }
   ],
   "source": [
    "# Summarize the model over the training set and print out some metrics\n",
    "trainingSummary = lrModel.summary\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|            features|               label|          prediction|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|[0.0,1.0,1.0,2014...|                 1.0|  0.8995087999842729|\n",
      "|[0.0,1.0,1.0,2014...| -1.0000000000000002| -0.5866447239642745|\n",
      "|[0.0,1.0,1.0,2014...|                 3.0|  2.2483556641043645|\n",
      "|[0.0,1.0,1.0,2014...|-0.33333333333333326|-0.11741386547255964|\n",
      "|[0.0,1.0,1.0,2014...|                2.75|  2.0209040947343007|\n",
      "|[0.0,1.0,1.0,2015...|                 0.0| 0.12210537305288557|\n",
      "|[0.0,1.0,1.0,2015...|                 1.0|  0.8995087999842729|\n",
      "|[0.0,1.0,2.0,2014...|                 1.0|  0.8995087999842729|\n",
      "|[0.0,1.0,2.0,2014...|                 0.0|  0.1662396565691747|\n",
      "|[0.0,1.0,2.0,2015...|                0.75|  0.6720572306142092|\n",
      "|[0.0,1.0,2.0,2015...|  2.3333333333333335|  1.7398943313759483|\n",
      "|[0.0,1.0,3.0,2014...|                -1.0| -0.5670294868459235|\n",
      "|[0.0,1.0,3.0,2014...|                 2.0|  1.5739322320443188|\n",
      "|[0.0,1.0,3.0,2014...|                 1.0|  0.8995087999842729|\n",
      "|[0.0,1.0,3.0,2014...|  2.6666666666666665|  1.8862411935892271|\n",
      "|[0.0,1.0,3.0,2014...|                 2.5|  1.9111439480743417|\n",
      "|[0.0,1.0,3.0,2014...|                 1.0|  0.8406630886292206|\n",
      "|[0.0,1.0,3.0,2015...|                 0.0|  0.1662396565691747|\n",
      "|[0.0,1.0,3.0,2015...|                 3.8|   2.787894409752401|\n",
      "|[0.0,1.0,3.0,2015...|                -8.0|  -5.817604913461716|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit on validation set\n",
    "validpredicts = lrModel.transform(valid_df)\n",
    "validpredicts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 0.56702\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(validpredicts)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generalized Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GeneralizedLinearRegression\n",
    "glr = GeneralizedLinearRegression(family=\"gaussian\", link=\"identity\", maxIter=10, regParam=0.3)\n",
    "# Fit the model\n",
    "glrmodel = glr.fit(lpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+--------------------+\n",
      "|            features|              label|          prediction|\n",
      "+--------------------+-------------------+--------------------+\n",
      "|[0.0,1.0,4.0,2014...|               -0.5|-0.13092343534083228|\n",
      "|[0.0,1.0,4.0,2014...|-0.6666666666666667|-0.49274437642551777|\n",
      "|[0.0,1.0,4.0,2014...| 0.6666666666666667|  0.5573660449098027|\n",
      "|[0.0,1.0,4.0,2014...|                1.0|  0.9459357108710079|\n",
      "|[0.0,1.0,4.0,2014...|                1.0|    0.95467990402772|\n",
      "|[0.0,1.0,4.0,2014...|                1.0|  0.9561703654777277|\n",
      "|[0.0,1.0,4.0,2014...|                1.0|  0.9075470802903581|\n",
      "|[0.0,1.0,4.0,2014...|               -0.5|-0.32919074983942453|\n",
      "|[0.0,1.0,4.0,2014...|                0.0| 0.05532153668088524|\n",
      "|[0.0,1.0,4.0,2014...|                0.0| 0.06662025170452501|\n",
      "|[0.0,1.0,4.0,2014...|               1.25|  0.9371638551499849|\n",
      "|[0.0,1.0,4.0,2014...|               -1.0| -0.7016509497861689|\n",
      "|[0.0,1.0,4.0,2014...|                0.0|   0.159911003054665|\n",
      "|[0.0,1.0,4.0,2014...|                1.0|  0.9549597386306422|\n",
      "|[0.0,1.0,4.0,2014...|                0.5| 0.46939399384318736|\n",
      "|[0.0,1.0,4.0,2014...| 1.6666666666666667|  1.3413788312581842|\n",
      "|[0.0,1.0,4.0,2014...|  2.916666666666667|   2.197973250460753|\n",
      "|[0.0,1.0,4.0,2014...|                1.0|  0.8423845486054035|\n",
      "|[0.0,1.0,4.0,2014...|                0.0| 0.05451604023050238|\n",
      "|[0.0,1.0,4.0,2014...|               0.75|  0.5843327402625972|\n",
      "+--------------------+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit on validation set\n",
    "validpredicts = glrmodel.transform(lpoints)\n",
    "validpredicts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "|-- weekend: integer (nullable = true)\n",
    " |-- weekday: integer (nullable = true)\n",
    " |-- month: integer (nullable = true)\n",
    " |-- year: integer (nullable = true)\n",
    " |-- station_id: integer (nullable = true)\n",
    " |-- hourofday: integer (nullable = true)\n",
    " |-- total_capacity: integer (nullable = true)\n",
    " |-- lat: double (nullable = true)\n",
    " |-- long: double (nullable = true)\n",
    " ....\n",
    " |-- dayofweek: double (nullable = true)\n",
    " |-- city: double (nullable = true)\n",
    " |-- station_name: double (nullable = true)\n",
    "\"\"\"\n",
    "\n",
    "traffic_predictions = validpredicts.select('prediction').collect()\n",
    "traffic_features = validpredicts.select('features').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(prediction=-0.7597863470875004)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traffic_predictions[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traffic_features[0][0][len(traffic_features[0][0])-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(traffic_features[0][0][])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction, station_id, lat, long, hourofday, month, year, dayofweek, city, station_name\n",
    "list_tupes = list()\n",
    "for i in range(len(traffic_features)):\n",
    "    list_tupes.append((traffic_predictions[i][0], traffic_features[i][0][4], traffic_features[i][0][7], traffic_features[i][0][8],\\\n",
    "                      traffic_features[i][0][5], traffic_features[i][0][2], traffic_features[i][0][3], \\\n",
    "                       traffic_features[i][0][len(traffic_features[0][0])-3],\\\n",
    "                      traffic_features[i][0][len(traffic_features[0][0])-2],\\\n",
    "                      traffic_features[i][0][len(traffic_features[0][0])-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 0.460254\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(validpredicts)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Prediction for availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_nodup_numeric_availability = final_nodup_numeric.drop('traffic', 'avg_docks_available')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_cols = final_nodup_numeric_availability.schema.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_cols.remove('avg_bikes_available')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merging the data with Vector Assembler.\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "va = VectorAssembler(outputCol=\"features\", inputCols = input_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#lpoints - labeled data.\n",
    "lpoints_availability = va.transform(final_nodup_numeric_availability).select(\"features\", \"avg_bikes_available\").withColumnRenamed(\"avg_bikes_available\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|            features|             label|\n",
      "+--------------------+------------------+\n",
      "|[0.0,1.0,4.0,2014...| 6.604166666666667|\n",
      "|[0.0,1.0,4.0,2014...|18.916666666666668|\n",
      "|[0.0,1.0,4.0,2014...| 4.204166666666667|\n",
      "|[0.0,1.0,4.0,2014...| 6.591666666666667|\n",
      "|[0.0,1.0,4.0,2014...| 9.316666666666666|\n",
      "|[0.0,1.0,4.0,2014...| 8.016666666666667|\n",
      "|[0.0,1.0,4.0,2014...| 4.920833333333333|\n",
      "|[0.0,1.0,4.0,2014...| 7.016666666666667|\n",
      "|[0.0,1.0,4.0,2014...| 6.170212765957447|\n",
      "|[0.0,1.0,4.0,2014...|             2.775|\n",
      "|[0.0,1.0,4.0,2014...|            18.775|\n",
      "|[0.0,1.0,4.0,2014...|              7.95|\n",
      "|[0.0,1.0,4.0,2014...| 6.729166666666667|\n",
      "|[0.0,1.0,4.0,2014...| 7.083333333333333|\n",
      "|[0.0,1.0,4.0,2014...|            5.7625|\n",
      "|[0.0,1.0,4.0,2014...|            6.1375|\n",
      "|[0.0,1.0,4.0,2014...|10.904166666666667|\n",
      "|[0.0,1.0,4.0,2014...|             6.475|\n",
      "|[0.0,1.0,4.0,2014...| 9.995833333333334|\n",
      "|[0.0,1.0,4.0,2014...|              10.5|\n",
      "+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lpoints_availability.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Divide the dataset into training and testing sets.\n",
    "splits_availability = lpoints_availability.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cache() : the algorithm is interative and training and data sets are going to be reused many times.\n",
    "train_df_availability = splits_availability[0].cache()\n",
    "valid_df_availability = splits_availability[1].cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression for Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Train the model.\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "lrModel_availability = lr.fit(lpoints_availability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 2.704588\n",
      "r2: 0.291949\n"
     ]
    }
   ],
   "source": [
    "# Summarize the model over the training set and print out some metrics\n",
    "trainingSummary_availability = lrModel_availability.summary\n",
    "print(\"RMSE: %f\" % trainingSummary_availability.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary_availability.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+--------------------+\n",
      "|            features|             label|          prediction|\n",
      "+--------------------+------------------+--------------------+\n",
      "|[0.0,1.0,1.0,2014...|14.043333333333333| 0.10739394521412235|\n",
      "|[0.0,1.0,1.0,2014...|15.133333333333333|  -0.398423628830912|\n",
      "|[0.0,1.0,1.0,2014...|              6.49| -0.9336640585534726|\n",
      "|[0.0,1.0,1.0,2014...| 6.733333333333333|  0.8995087999842729|\n",
      "|[0.0,1.0,1.0,2015...| 9.856666666666667|  0.8995087999842729|\n",
      "|[0.0,1.0,2.0,2014...| 7.191666666666666|  0.1662396565691747|\n",
      "|[0.0,1.0,2.0,2014...|              6.15|  1.9111439480743417|\n",
      "|[0.0,1.0,2.0,2014...|           17.7375| -1.9385190548549236|\n",
      "|[0.0,1.0,2.0,2014...|10.720833333333333| -0.8702982460060087|\n",
      "|[0.0,1.0,2.0,2014...|12.070833333333333|   -1.11020119261863|\n",
      "|[0.0,1.0,2.0,2015...|            4.2875|-0.07818339123585796|\n",
      "|[0.0,1.0,2.0,2015...|10.645833333333334| -0.2003949151383743|\n",
      "|[0.0,1.0,2.0,2015...| 6.658333333333333| -0.7141437652335543|\n",
      "|[0.0,1.0,2.0,2015...|10.279166666666667| -3.9040947639415373|\n",
      "|[0.0,1.0,3.0,2014...| 7.091666666666667|  0.1662396565691747|\n",
      "|[0.0,1.0,3.0,2014...| 8.776666666666667|-0.19587481394596298|\n",
      "|[0.0,1.0,3.0,2015...| 4.291666666666667|  0.8995087999842729|\n",
      "|[0.0,1.0,3.0,2015...| 7.716666666666667|  0.8995087999842729|\n",
      "|[0.0,1.0,4.0,2014...| 9.956666666666667|  0.8995087999842729|\n",
      "|[0.0,1.0,4.0,2014...|               8.1| 0.10739394521412235|\n",
      "+--------------------+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit on validation set\n",
    "validpredicts_availability = lrModel.transform(valid_df_availability)\n",
    "validpredicts_availability.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalized Linear Regression for Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GeneralizedLinearRegression\n",
    "glr = GeneralizedLinearRegression(family=\"gaussian\", link=\"identity\", maxIter=10, regParam=0.3)\n",
    "# Fit the model\n",
    "glrmodel_availability = glr.fit(train_df_availability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+--------------------+\n",
      "|            features|             label|          prediction|\n",
      "+--------------------+------------------+--------------------+\n",
      "|[0.0,1.0,1.0,2014...|14.043333333333333| 0.14919182670136166|\n",
      "|[0.0,1.0,1.0,2014...|15.133333333333333| -0.5037824354786058|\n",
      "|[0.0,1.0,1.0,2014...|              6.49|  -1.117434153350874|\n",
      "|[0.0,1.0,1.0,2014...| 6.733333333333333|  0.8577138474827808|\n",
      "|[0.0,1.0,1.0,2015...| 9.856666666666667|  0.8347164154325073|\n",
      "|[0.0,1.0,2.0,2014...| 7.191666666666666| 0.14265552048899943|\n",
      "|[0.0,1.0,2.0,2014...|              6.15|  1.9945993346308954|\n",
      "|[0.0,1.0,2.0,2014...|           17.7375|  -2.078060191849284|\n",
      "|[0.0,1.0,2.0,2014...|10.720833333333333| -0.9543413699910148|\n",
      "|[0.0,1.0,2.0,2014...|12.070833333333333| -1.1925562869292394|\n",
      "|[0.0,1.0,2.0,2015...|            4.2875|-0.20203965741448426|\n",
      "|[0.0,1.0,2.0,2015...|10.645833333333334|-0.24229871945506565|\n",
      "|[0.0,1.0,2.0,2015...| 6.658333333333333| -0.7185371084394045|\n",
      "|[0.0,1.0,2.0,2015...|10.279166666666667|  -4.057085184848448|\n",
      "|[0.0,1.0,3.0,2014...| 7.091666666666667| 0.13922925365429295|\n",
      "|[0.0,1.0,3.0,2014...| 8.776666666666667|-0.20413388894560658|\n",
      "|[0.0,1.0,3.0,2015...| 4.291666666666667|  0.8980975011785781|\n",
      "|[0.0,1.0,3.0,2015...| 7.716666666666667|  0.9908355025144782|\n",
      "|[0.0,1.0,4.0,2014...| 9.956666666666667|  0.8854181506822445|\n",
      "|[0.0,1.0,4.0,2014...|               8.1|-0.00741623329089...|\n",
      "+--------------------+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit on validation set\n",
    "validpredicts_availability = glrmodel.transform(valid_df_availability)\n",
    "validpredicts_availability.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest for availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(maxBins = 70)\n",
    "rfmodel = rf.fit(lpoints_availability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rfpredicts = rfmodel.transform(lpoints_availability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select example rows to display.\n",
    "availability_features = rfpredicts.select(\"features\").collect()\n",
    "availability_prediction = rfpredicts.select(\"label\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 2.43219\n"
     ]
    }
   ],
   "source": [
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(rfpredicts)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120757, 120757, 120757)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(availability_features), len(availability_prediction), len(traffic_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "availability_features[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction_available, prediction_traffic, station_id, lat, long, hourofday, month, year, dayofweek, city, station_name\n",
    "available_traffic_pred_list = list()\n",
    "for i in range(len(availability_features)):\n",
    "    available_traffic_pred_list.append((availability_prediction[i][0],traffic_predictions[i][0],\\\n",
    "                                availability_features[i][0][4], \\\n",
    "                       availability_features[i][0][7], availability_features[i][0][8],\\\n",
    "                      availability_features[i][0][5], availability_features[i][0][2], \\\n",
    "                       availability_features[i][0][3], \\\n",
    "                       availability_features[i][0][len(availability_features[0][0])-3],\\\n",
    "                      availability_features[i][0][len(availability_features[0][0])-2],\\\n",
    "                      availability_features[i][0][len(availability_features[0][0])-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(available_traffic_pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['prediction_available', 'prediction_traffic', 'station_id', 'lat', 'long', 'hourofday', 'month', 'year', 'dayofweek', 'city', 'station_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "station_pd = pd.read_csv('../bikeshare/Data/station.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>dock_count</th>\n",
       "      <th>city</th>\n",
       "      <th>installation_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>San Jose Diridon Caltrain Station</td>\n",
       "      <td>37.329732</td>\n",
       "      <td>-121.901782</td>\n",
       "      <td>27</td>\n",
       "      <td>San Jose</td>\n",
       "      <td>8/6/2013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                               name        lat        long  dock_count  \\\n",
       "0   2  San Jose Diridon Caltrain Station  37.329732 -121.901782          27   \n",
       "\n",
       "       city installation_date  \n",
       "0  San Jose          8/6/2013  "
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "station_pd[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df.merge(station_pd, left_on='station_id', right_on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = new_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv('available_traffic.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Beale at Market\n",
    "\n",
    "Temporary Transbay Terminal (Howard at Beale)\n",
    "\n",
    "Steuart at Market\n",
    "\n",
    "Spear at Folsom\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_station_df = new_df[(new_df['name'].str.contains('Beale at Market', case = False)) \\\n",
    "       & (new_df['month'] == 8.0) & (new_df['year'] == 2015.0)\n",
    "      & (new_df['dayofweek'] == 3.0) & (new_df['hourofday'] == 10.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_station_df = nearest_station_df.append(new_df[(new_df['name'].str.contains('Howard at Beale', case = False)) \\\n",
    "       & (new_df['month'] == 8.0) & (new_df['year'] == 2015.0)\\\n",
    "      & (new_df['dayofweek'] == 3.0) & (new_df['hourofday'] == 10.0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_station_df = nearest_station_df.append(new_df[(new_df['name'].str.contains('Steuart at Market', case = False)) \\\n",
    "       & (new_df['month'] == 8.0) & (new_df['year'] == 2015.0)\\\n",
    "      & (new_df['dayofweek'] == 3.0) & (new_df['hourofday'] == 10.0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_station_df = nearest_station_df.append(new_df[(new_df['name'].str.contains('Spear at Folsom', case = False)) \\\n",
    "       & (new_df['month'] == 8.0) & (new_df['year'] == 2015.0)\\\n",
    "      & (new_df['dayofweek'] == 3.0) & (new_df['hourofday'] == 10.0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_station_df.to_csv('nearest_station_df.csv')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
